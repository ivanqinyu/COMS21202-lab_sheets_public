{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Evaluating a classifier\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab we will continue our introduction to data classification. Specifically, we will see how to evaluate a classifier using a more analytical approach, using two standard metrics for classification: __accuracy__ and __confusion matrix__. Recall that in lab 5 we used a __qualitative__ approach to analyse the results. As promised then, we will take a __quantitative__ look at the results today, i.e. we will use some metric that quantifies the performance of the classifier, as opposed to rely on a graphical interpretation of the results.\n",
    "\n",
    "__Note__: you will need to calculate both accuracy and confusion matrix for Coursework 2. You should thus try to write your code as neat and reusable as you can (i. e. use functions!).\n",
    "\n",
    "As usual, let's import the libraries before we start by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # to avoid issues between Python 2 and 3 printing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# show matplotlib figures inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default we set figures to be 12\"x8\" on a 110 dots per inch (DPI) screen \n",
    "# (adjust DPI if you have a high res screen!)\n",
    "plt.rc('figure', figsize=(12, 8), dpi=110)\n",
    "plt.rc('font', size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "In this lab we will use the Iris dataset again. Let's run the cell below to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris train and test sets\n",
    "\n",
    "def load_iris_data(train_path='iris_train.csv', test_path='iris_test.csv'):\n",
    "    train_set = np.loadtxt(train_path, delimiter=',')\n",
    "    test_set = np.loadtxt(test_path, delimiter=',')\n",
    "\n",
    "    # separate labels from features\n",
    "    train_labels = train_set[:, 4].astype(np.int)\n",
    "    train_set = train_set[:, 0:4]\n",
    "    test_labels = test_set[:, 4].astype(np.int)\n",
    "    test_set = test_set[:, 0:4]\n",
    "    \n",
    "    return train_labels, train_set, test_labels, test_set\n",
    "\n",
    "train_labels, train_set, test_labels, test_set = load_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load your results\n",
    "\n",
    "Load now the results you obtained in lab 5 with your Nearest-Centroid classifier. These should be stored as a single CSV file containing the class predicted by your classifier for each sample in the test set. \n",
    "\n",
    "You can use [`np.loadtxt(path, delimiter=',', dtype=np.int`)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html#numpy.loadtxt) to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accuracy\n",
    "\n",
    "Accuracy is an intuitive standard metric to evaluate the performance of a classifier. \n",
    "\n",
    "Calculating the accuracy of a classifier simply corresponds to calculate the __percentage of correctly classified samples__. Naturally, a sample is correctly classified when its ground truth class is the same as the class predicted by the classifier.\n",
    "\n",
    "Now that you have loaded both the test labels and your predictions, calculate and print the accuracy of your classifier. You will need to calculate the accuracy for coursework 2, so it's best to write a function to do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(gt_labels, pred_labels):\n",
    "    # write your code here (remember to return the accuracy at the end!)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix\n",
    "\n",
    "Accuracy tells us how well a classifier is performing __overall__. However, it doesn't tell us how good the classifier is at recognising the various classes in the dataset. \n",
    "\n",
    "Consider the following example: let's suppose we have 10 test samples belonging to three classes. Let's suppose the ground truth test labels are the following:\n",
    "\n",
    "```python\n",
    "gt = [0, 0, 0, 0, 0, 0, 0, 1, 1, 2]\n",
    "```\n",
    "\n",
    "As you can notice, the test set is quite unbalanced, since we have 7/10 samples belonging to class `0`, 2/10 samples belonging to class `1` and only 1/10 belonging to class `2`. Though this is just a simple example, it resembles a common case. Let's now suppose our classifier predicted the following class labels for our toy test set:\n",
    "\n",
    "```python\n",
    "pred = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "So our classifier is classifying every sample as a class `0` sample. Let's calculate the accuracy now: since there are 7/10 samples belonging to class 0 in the test set, the accuracy of the classifier is 70%! Surprisingly high for a classifier that predicts nothing but class `0`, right? \n",
    "\n",
    "Without looking at the actual predictions of the classifier we would thus overlook our results. This is when the __confusion matrix__ comes in handy. \n",
    "\n",
    "### Definition\n",
    "\n",
    "The confusion matrix is a square matrix of size $N_c x N_c$, where $N_c$ is the number of classes. The confusion matrix $CM$ is constructed as follows:\n",
    "\n",
    "$$\n",
    "CM(i, j) = \\frac{\\text{# of samples belonging to class $i$ that were classified as class $j$}}{\\text{# of samples belonging to class $i$}} \\\\\n",
    "$$\n",
    "\n",
    "Where $i, j \\in \\{1, \\dots, N_c\\}$. The elements along the main diagonal correspond to the percentages of correctly classified samples, for each class, while elements outside the main diagional identify the error of the classifier. Thus, a perfect classifier would get an identity matrix. Notice that, by definition, the sum over the elements of each row equals 1.\n",
    "\n",
    "For the above example, then, the confusion matrix would be the following:\n",
    "\n",
    "<img src=\"cm.png\" width=\"250\"/>\n",
    "\n",
    "By looking at the matrix, we can have an immediate understanding as to how the classifier _confuses_ the classes, without having to check the predicted class of each sample. In fact, by looking at the above matrix, we can see that all the test samples are classified as class `0` (first column is all ones). This confusion matrix doesn't look great and tells us our classifier is doing a poor job since it's ignoring 2 out of the 3 classes (but still, accuracy is 70%!). \n",
    "\n",
    "Confusion matrices are commonly used to gain a deeper understanding about the performance of a classifier and identify potential issues with specific classes.\n",
    "\n",
    "Let's code now a little bit. Again, since you will need to calculate the confusion matrix for coursework 2, let's create a function that we can reuse later. Calculate and print the confusion matrix given your results. Recall that the sum over the elements of each row must be 1, so double check that in order to make sure your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(gt_labels, pred_labels):\n",
    "    # write your code here (remember to return the confusion matrix at the end!)       \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the confusion matrix\n",
    "\n",
    "Let's now create a function that plots the confusion matrix as a colour image. This is a common way to display confusion matrices since the colour scales aids the interpretation of the confusion matrix.\n",
    "\n",
    "You should plot the confusion matrix you calculated before with your own results. You should obtain something like this:\n",
    "\n",
    "![](cm_iris.png)\n",
    "\n",
    "Yet again, we want to reuse our code as much as possible, so we will create a function that will plot _any given two-dimensional matrix_ given in input:\n",
    "\n",
    "__Hints__:\n",
    "\n",
    "- Use Matplolib's function [`imshow(X)`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html#matplotlib.pyplot.imshow)\n",
    "    - This function takes an optional parameter `cmap` which sets the colour map to be used for the plot. We set `cmap=plt.get_cmap('summer')`. Feel free to try a few different ones to suit your taste. You can read more about colour maps in Matplotlib [here](https://matplotlib.org/tutorials/colors/colormaps.html)\n",
    "    - This function returns an handle to the plotted image\n",
    "- Use Matplotlib's function [`colorbar(handle)`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.colorbar.html) to display the side colour bar. You should pass the aforementioned handle returned by `imshow`\n",
    "- Use Matplotlib's function [`text`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.text.html) to show the matrix's numbers\n",
    "\n",
    "We are providing a little code stub to get you started. Note the text enclosed by triple double quotes `\"\"\"`. That text is a short documentation to the function that would be displayed by any decent IDE. You can read more about functions documentations [here](https://www.python.org/dev/peps/pep-0257/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(matrix, ax=None):\n",
    "    \"\"\"\n",
    "    Displays a given matrix as an image.\n",
    "    \n",
    "    Args:\n",
    "        - matrix: the matrix to be displayed        \n",
    "        - ax: the matplotlib axis where to overlay the plot. \n",
    "          If you create the figure with `fig, fig_ax = plt.subplots()` simply pass `ax=fig_ax`. \n",
    "          If you do not explicitily create a figure, then pass no extra argument.  \n",
    "          In this case the  current axis (i.e. `plt.gca())` will be used        \n",
    "    \"\"\"    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    # write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Iris features evaluation\n",
    "\n",
    "Let's take a step back now. Recall from our previous lab session that we manually selected two out of the four features to classify the iris flowers into the three different classes.\n",
    "\n",
    "Let's try not to do that now. We will instead evaluate all the pairwise feature combinations. In other words, we want you to __run again your Nearest-Centroid classifier on all the pairwise feature combinations__.\n",
    "\n",
    "Note that we have already loaded the data in this notebook. If you wrote functions in lab 5 (yay!) you should simply be able to copy and paste your function here to run the classifier again on all the features combination.\n",
    "\n",
    "Create a 4x4 matrix and store at position (i, j) the __accuracy__ you obtained with your classifier using features i and j.\n",
    "\n",
    "Finally, plot the 4x4 matrix using the function `plot_matrix` you just coded to see how different features affect the performance of the classifier. Hopefully by now it should be clear why writing reusable code is very useful: we created a function that calculates the accuracy given the prediction of a classifier, and we created a function that plots any given 2D matrix. With very little code thus we can implement this last bit of the sheet.\n",
    "\n",
    "You should obtain a plot similar to the one below.\n",
    "\n",
    "![](results_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here (remember to add the needed code from lab 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compare the qualitative approach we took in lab 5 to the quantitative approach we took here. \n",
    "\n",
    "__What are the benefits of using each approach?__ \n",
    "\n",
    "Notice that both approaches are valid and commonly used when evaluating a classifier or any machine learning algorithm in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "A qualitative approach provides generally speaking a more understandable interpretation of the results. \n",
    "\n",
    "For example, when we plotted the Voronoi diagram over the test set in lab 5, we could see the decision boundaries of the classifier, as well as how the data was spread. In fact, we could observe that compact clusters (class 1) are easily classified, given that the data is distributed close to the centroid. When a class exhibits a larger intra-varation, instead, a simple classifier like the Nearest-Centroid might struggle.\n",
    "\n",
    "A quantitative approach helps to compare different solutions in a more rigorous way. By having a well defined metric like accuracy, we could directly evaluate how different features affect the classifier. Doing this in a qualitative way, e.g. by plotting the test points and the Voronoi diagram for each feature combination, would have been rather cumbersome. \n",
    "\n",
    "By and large, then, we can say that a qualitative analysis helps understanding the behaviour of an algorithm, while a quantitative analysis helps in defining an objective measure of the performance that can be used to directly compare different solutions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
